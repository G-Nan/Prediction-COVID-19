{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T05:55:36.131696Z",
     "start_time": "2023-06-28T05:55:34.446417Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4764,
     "status": "ok",
     "timestamp": 1687885686326,
     "user": {
      "displayName": "서민경",
      "userId": "09611163852089986535"
     },
     "user_tz": -540
    },
    "id": "pFhy95XbZqOS",
    "outputId": "b5f85522-3e18-497d-dd51-52996b16820b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1eb32bdeaf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn as nn\n",
    "from numpy import genfromtxt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T05:55:46.074516Z",
     "start_time": "2023-06-28T05:55:46.062566Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1687885701199,
     "user": {
      "displayName": "서민경",
      "userId": "09611163852089986535"
     },
     "user_tz": -540
    },
    "id": "9j9jpheSfGwv"
   },
   "outputs": [],
   "source": [
    "# city\n",
    "keys = ['경기','강원','경북','경남','전북','전남','충북','충남',\n",
    "        '서울','대전','광주','대구','울산','부산','인천','세종','제주']\n",
    "values = ['Gyeonggi','Gangwon','Gyeongbuk','Gyeongnam','Jeonbuk','Jeonnam','Chungbuk','Chungnam',\n",
    "          'Seoul','Daejeon','Gwangju','Daegu','Ulsan','Busan','Incheon','Sejong','Jeju']\n",
    "eng = dict(zip(keys, values))\n",
    "\n",
    "# population\n",
    "list_N = [1539277, 13527299, 3311631, 2622174,\n",
    "          1440908, 2389150, 1454068, 3353379,\n",
    "          9535432, 370439, 1122757, 2952839,\n",
    "          1834016, 1786855, 676518, 2121108, 1597774]\n",
    "keys.sort()\n",
    "pop_N = dict(zip(keys, list_N))\n",
    "Ds=24 # 학습용 및 예측 경계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1687886409289,
     "user": {
      "displayName": "서민경",
      "userId": "09611163852089986535"
     },
     "user_tz": -540
    },
    "id": "AD6iFgYfZqOa",
    "outputId": "f3c47e49-01c8-4ac9-efc2-2cdcab526186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Parser   : 109 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "class DINN(nn.Module):\n",
    "    def __init__(self, t, S_data, I_data1, I_data2, D_data1, R_data1, D_data2, R_data2, alpha1): #[t,S,I1,I2,D,R,a1]\n",
    "        super(DINN, self).__init__()\n",
    "        self.N = pop_N[keys[i]] #population size\n",
    "        self.t = torch.tensor(t, requires_grad=True)\n",
    "        self.t_float = self.t.float()\n",
    "        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.I1 = torch.tensor(I_data1)\n",
    "        self.I2 = torch.tensor(I_data2)\n",
    "        self.D1 = torch.tensor(D_data1)\n",
    "        self.R1 = torch.tensor(R_data1)\n",
    "        self.D2 = torch.tensor(D_data2)\n",
    "        self.R2 = torch.tensor(R_data2)\n",
    "\n",
    "        self.losses = []\n",
    "        self.save = 3 #which file to save to\n",
    "\n",
    "        self.alpha1_tilda = torch.tensor(alpha1)\n",
    "        self.beta1_tilda = torch.tensor(0.0714)\n",
    "        self.gamma1_tilda = torch.tensor(0.008)\n",
    "\n",
    "        self.alpha2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "\n",
    "        # find values for normalization\n",
    "        # max\n",
    "        self.S_max = max(self.S)\n",
    "        self.I1_max = max(self.I1)\n",
    "        self.I2_max = max(self.I2)\n",
    "        self.D1_max = max(self.D1)\n",
    "        self.R1_max = max(self.R1)\n",
    "        self.D2_max = max(self.D2)\n",
    "        self.R2_max = max(self.R2)\n",
    "        # min\n",
    "        self.S_min = min(self.S)\n",
    "        self.I1_min = min(self.I1)\n",
    "        self.I2_min = min(self.I2)\n",
    "        self.D1_min = min(self.D1)\n",
    "        self.R1_min = min(self.R1)\n",
    "        self.D2_min = min(self.D2)\n",
    "        self.R2_min = min(self.R2)\n",
    "\n",
    "        # normalize\n",
    "        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
    "        self.I1_hat = (self.I1 - self.I1_min) / (self.I1_max - self.I1_min)\n",
    "        self.I2_hat = (self.I2 - self.I2_min) / (self.I2_max - self.I2_min)\n",
    "        self.D1_hat = (self.D1 - self.D1_min) / (self.D1_max - self.D1_min)\n",
    "        self.R1_hat = (self.R1 - self.R1_min) / (self.R1_max - self.R1_min)\n",
    "        self.D2_hat = (self.D2 - self.D2_min) / (self.D2_max - self.D2_min)\n",
    "        self.R2_hat = (self.R2 - self.R2_min) / (self.R2_max - self.R2_min)\n",
    "\n",
    "        # matrices (x4 for S,I1,I2,D,R,a1,a2) for the gradients\n",
    "        self.m1 = torch.zeros((len(self.t), 7)); self.m1[:, 0] = 1\n",
    "        self.m2 = torch.zeros((len(self.t), 7)); self.m2[:, 1] = 1\n",
    "        self.m3 = torch.zeros((len(self.t), 7)); self.m3[:, 2] = 1\n",
    "        self.m4 = torch.zeros((len(self.t), 7)); self.m4[:, 3] = 1\n",
    "        self.m5 = torch.zeros((len(self.t), 7)); self.m5[:, 4] = 1\n",
    "        self.m6 = torch.zeros((len(self.t), 7)); self.m6[:, 5] = 1\n",
    "        self.m7 = torch.zeros((len(self.t), 7)); self.m7[:, 6] = 1\n",
    "\n",
    "        # NN\n",
    "        self.net_sidr = self.Net_sidr()\n",
    "        self.params = list(self.net_sidr.parameters())\n",
    "        self.params.extend(list([self.alpha2_tilda, self.beta2_tilda, self.gamma2_tilda]))\n",
    "\n",
    "    # force parameters to be in a range\n",
    "    @property\n",
    "    def alpha1(self):\n",
    "        return self.alpha1_tilda\n",
    "\n",
    "    @property\n",
    "    def beta1(self):\n",
    "        return self.beta1_tilda\n",
    "    \n",
    "    @property\n",
    "    def gamma1(self):\n",
    "        return self.gamma1_tilda\n",
    "\n",
    "    @property\n",
    "    def alpha2(self):\n",
    "        return torch.tanh(self.alpha2_tilda)\n",
    "\n",
    "    @property\n",
    "    def beta2(self):\n",
    "        return torch.tanh(self.beta2_tilda)\n",
    "\n",
    "    @property\n",
    "    def gamma2(self):\n",
    "        return torch.tanh(self.gamma2_tilda)\n",
    "\n",
    "\n",
    "    #nets\n",
    "    class Net_sidr(nn.Module): # input = [t]\n",
    "        def __init__(self):\n",
    "            super(DINN.Net_sidr, self).__init__()\n",
    "            self.fc1=nn.Linear(1, 64)\n",
    "            self.fc2=nn.Linear(64, 64)\n",
    "            self.fc3=nn.Linear(64, 64)\n",
    "            self.fc4=nn.Linear(64, 64)\n",
    "            self.fc5=nn.Linear(64, 64)\n",
    "            self.fc6=nn.Linear(64, 64)\n",
    "            self.fc7=nn.Linear(64, 64)\n",
    "            self.fc8=nn.Linear(64, 64)\n",
    "            self.out=nn.Linear(64, 7) #outputs S, I1, I2, D, R\n",
    "\n",
    "        def forward(self, t_batch):\n",
    "            sidr=F.gelu(self.fc1(t_batch))\n",
    "            sidr=F.gelu(self.fc2(sidr))\n",
    "            sidr=F.gelu(self.fc3(sidr))\n",
    "            sidr=F.gelu(self.fc4(sidr))\n",
    "            sidr=F.gelu(self.fc5(sidr))\n",
    "            sidr=F.gelu(self.fc6(sidr))\n",
    "            sidr=F.gelu(self.fc7(sidr))\n",
    "            sidr=F.gelu(self.fc8(sidr))\n",
    "            sidr=self.out(sidr)\n",
    "            return sidr\n",
    "\n",
    "    def net_f(self, t_batch):\n",
    "        sidr_hat = self.net_sidr(t_batch)\n",
    "\n",
    "        S_hat, I1_hat, I2_hat, D1_hat, R1_hat, D2_hat, R2_hat \\\n",
    "            = sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3], sidr_hat[:,4], sidr_hat[:,5], sidr_hat[:,6]\n",
    "\n",
    "        #S_t\n",
    "        sidr_hat.backward(self.m1, retain_graph=True)\n",
    "\n",
    "        S_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #I1_t\n",
    "        sidr_hat.backward(self.m2, retain_graph=True)\n",
    "\n",
    "        I1_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #I2_t\n",
    "        sidr_hat.backward(self.m3, retain_graph=True)\n",
    "\n",
    "        I2_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #D1_t\n",
    "        sidr_hat.backward(self.m4, retain_graph=True)\n",
    "\n",
    "        D1_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #R1_t\n",
    "        sidr_hat.backward(self.m5, retain_graph=True)\n",
    "\n",
    "        R1_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #D2_t\n",
    "        sidr_hat.backward(self.m6, retain_graph=True)\n",
    "\n",
    "        D2_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "        #R2_t\n",
    "        sidr_hat.backward(self.m7, retain_graph=True)\n",
    "\n",
    "        R2_hat_t = self.t.grad.clone()\n",
    "        self.t.grad.zero_()\n",
    "\n",
    "\n",
    "        # unnormalize\n",
    "        S = self.S_min + (self.S_max - self.S_min) * S_hat\n",
    "        I1 = self.I1_min + (self.I1_max - self.I1_min) * I1_hat\n",
    "        I2 = self.I2_min + (self.I2_max - self.I2_min) * I2_hat\n",
    "        D1 = self.D1_min + (self.D1_max - self.D1_min) * D1_hat\n",
    "        R1 = self.R1_min + (self.R1_max - self.R1_min) * R1_hat\n",
    "        D2 = self.D2_min + (self.D2_max - self.D2_min) * D2_hat\n",
    "        R2 = self.R2_min + (self.R2_max - self.R2_min) * R2_hat\n",
    "\n",
    "        f1_hat = S_hat_t - (-(self.alpha1 / self.N) * S * I1 - (self.alpha2 / self.N) * S * I2)  / (self.S_max - self.S_min)\n",
    "        f2_hat = I1_hat_t - ((self.alpha1 / self.N) * S * I1 - self.beta1 * I1 - self.gamma1 * I1 ) / (self.I1_max - self.I1_min)\n",
    "        f3_hat = I2_hat_t - ((self.alpha2 / self.N) * S * I2 - self.beta2 * I2 - self.gamma2 * I2 ) / (self.I2_max - self.I2_min)\n",
    "        f4_hat = D1_hat_t - (self.gamma1 * I1) / (self.D1_max - self.D1_min)\n",
    "        f5_hat = R1_hat_t - (self.beta1 * I1) / (self.R1_max - self.R1_min)\n",
    "        f6_hat = D2_hat_t - (self.gamma2 * I2) / (self.D2_max - self.D2_min)\n",
    "        f7_hat = R2_hat_t - (self.beta2 * I2) / (self.R2_max - self.R2_min)\n",
    "\n",
    "\n",
    "\n",
    "        return f1_hat, f2_hat, f3_hat, f4_hat, f5_hat, f6_hat, f7_hat, S_hat, I1_hat, I2_hat, D1_hat, R1_hat, D2_hat, R2_hat\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        # Load checkpoint\n",
    "        try:\n",
    "            checkpoint = torch.load(PATH + str(self.save)+'.pt')\n",
    "            # print('\\nloading pre-trained model...')\n",
    "            self.load_state_dict(checkpoint['model'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "            epoch = checkpoint['epoch']\n",
    "\n",
    "            self.losses = checkpoint['losses']\n",
    "\n",
    "\n",
    "        except RuntimeError :\n",
    "            #   print('changed the architecture, ignore')\n",
    "            pass\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        #try loading\n",
    "        self.load()\n",
    "\n",
    "        #train\n",
    "        print(f'\\nstarting training_{keys[i]}...\\n')\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            #lists to hold the output (maintain only the final epoch)\n",
    "            S_pred_list = []\n",
    "            I1_pred_list = []\n",
    "            I2_pred_list = []\n",
    "            D1_pred_list = []\n",
    "            R1_pred_list = []\n",
    "            D2_pred_list = []\n",
    "            R2_pred_list = []\n",
    "\n",
    "\n",
    "            f1, f2, f3, f4, f5, f6, f7, S_pred, I1_pred, \\\n",
    "                I2_pred, D1_pred, R1_pred, D2_pred, R2_pred = self.net_f(self.t_batch)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            S_pred_list.append(self.S_min + (self.S_max - self.S_min) * S_pred)\n",
    "            I1_pred_list.append(self.I1_min + (self.I1_max - self.I1_min) * I1_pred)\n",
    "            I2_pred_list.append(self.I2_min + (self.I2_max - self.I2_min) * I2_pred)\n",
    "            D1_pred_list.append(self.D1_min + (self.D1_max - self.D1_min) * D1_pred)\n",
    "            R1_pred_list.append(self.R1_min + (self.R1_max - self.R1_min) * R1_pred)\n",
    "            D2_pred_list.append(self.D2_min + (self.D2_max - self.D2_min) * D2_pred)\n",
    "            R2_pred_list.append(self.R2_min + (self.R2_max - self.R2_min) * R2_pred)\n",
    "\n",
    "\n",
    "            # loss\n",
    "            loss = (torch.mean(torch.square(self.S_hat[:Ds] - S_pred[:Ds]))+\n",
    "                        torch.mean(torch.square(self.I1_hat[:Ds] - I1_pred[:Ds]))+\n",
    "                        torch.mean(torch.square(self.I2_hat[:Ds] - I2_pred[:Ds]))+\n",
    "                        torch.mean(torch.square(self.D1_hat[:Ds] - D1_pred[:Ds]))+\n",
    "                        torch.mean(torch.square(self.R1_hat[:Ds] - R1_pred[:Ds]))+\n",
    "                        torch.mean(torch.square(self.D2_hat[:Ds] - D2_pred[:Ds]))+\n",
    "                        torch.mean(torch.square(self.R2_hat[:Ds] - R2_pred[:Ds]))+\n",
    "                        torch.mean(torch.square(f1[:Ds]))+\n",
    "                        torch.mean(torch.square(f2[:Ds]))+\n",
    "                        torch.mean(torch.square(f3[:Ds]))+\n",
    "                        torch.mean(torch.square(f4[:Ds]))+\n",
    "                        torch.mean(torch.square(f5[:Ds]))+\n",
    "                        torch.mean(torch.square(f6[:Ds]))+\n",
    "                        torch.mean(torch.square(f7[:Ds]))\n",
    "                        )\n",
    "\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "\n",
    "\n",
    "            self.losses.append(loss.item())\n",
    "\n",
    "            # if epoch % 1000 == 0:\n",
    "            #   print('\\nEpoch ', epoch)\n",
    "\n",
    "            #loss + model parameters update\n",
    "            if epoch % 4000 == 0:\n",
    "              #checkpoint save every 100 epochs if the loss is lower\n",
    "        #   print('\\nSaving model... Loss is: ', loss)\n",
    "          torch.save({\n",
    "              'epoch': epoch,\n",
    "              'model': self.state_dict(),\n",
    "              'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "              'scheduler': self.scheduler.state_dict(),\n",
    "              #'loss': loss,\n",
    "              'losses': self.losses,\n",
    "              }, PATH + str(self.save)+'.pt')\n",
    "          if self.save % 2 > 0: #its on 3\n",
    "            self.save = 2 #change to 2\n",
    "          else: #its on 2\n",
    "            self.save = 3 #change to 3\n",
    "\n",
    "        #   print('epoch: ', epoch)\n",
    "\n",
    "\n",
    "      return S_pred_list, I1_pred_list, I2_pred_list, D1_pred_list, R1_pred_list, D2_pred_list, R2_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1687886412708,
     "user": {
      "displayName": "서민경",
      "userId": "09611163852089986535"
     },
     "user_tz": -540
    },
    "id": "KAIs84CFdMPR",
    "outputId": "8264d4b3-ba19-417c-ff47-8305e6db4581"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_강원...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1/17 [25:13<6:43:40, 1513.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_경기...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2/17 [51:15<6:25:26, 1541.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_경남...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 3/17 [1:17:47<6:05:10, 1565.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_경북...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 4/17 [1:44:00<5:39:44, 1568.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_광주...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 5/17 [2:08:14<5:05:24, 1527.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_대구...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 6/17 [2:31:12<4:30:37, 1476.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_대전...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 7/17 [2:54:59<4:03:21, 1460.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_부산...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 8/17 [3:18:12<3:35:49, 1438.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_서울...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 9/17 [3:41:10<3:09:18, 1419.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_세종...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 10/17 [4:04:59<2:45:57, 1422.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_울산...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 11/17 [4:28:06<2:21:10, 1411.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_인천...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 12/17 [4:51:29<1:57:25, 1409.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_전남...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 13/17 [5:15:54<1:35:03, 1425.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_전북...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 14/17 [5:39:53<1:11:29, 1429.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_제주...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 15/17 [6:03:54<47:46, 1433.19s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_충남...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 16/17 [6:27:56<23:55, 1435.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training_충북...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [6:51:19<00:00, 1451.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1d 14h 35min 25s\n",
      "Wall time: 6h 51min 19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in tqdm(range(len(keys))):\n",
    "    print('\\n')\n",
    "    PATH = f'./model/covid_real_data_cumulative_cases64{eng[keys[i]]}'\n",
    "    covid_cumulative_cases = pd.read_csv(f'../data/Variants/SIR/delta_omicron/{keys[i]}_delta_omicron.csv', delimiter=',') #in the form of [t,S,I,D,R]\n",
    "    covid_cumulative_cases.columns = ['Date', 'cumulative_susceptible', 'cumulative_infected1', 'cumulative_recovered1', 'cumulative_dead1', 'alpha1', 'beta1', 'gamma1',\n",
    "                                    'cumulative_infected2', 'cumulative_recovered2', 'cumulative_dead2', 'alpha2', 'beta2', 'gamma2'] #rename columns\n",
    "    covid_cumulative_cases = covid_cumulative_cases[100:289].copy() # 2022-01-05 예측 (24주 학습 / 3주 예측)\n",
    "    covid_cumulative_cases.reset_index(drop=True,inplace=True)\n",
    "    covid_cumulative_cases['t'] = covid_cumulative_cases.index.astype(float)\n",
    "\n",
    "    #take only a sample of 31\n",
    "    cumulative_susceptible = []\n",
    "    cumulative_infected1 = []\n",
    "    cumulative_infected2 = []\n",
    "    cumulative_dead1 = []\n",
    "    cumulative_recovered1 = []\n",
    "    cumulative_dead2 = []\n",
    "    cumulative_recovered2 = []\n",
    "    alpha1 = []\n",
    "\n",
    "    timesteps = []\n",
    "\n",
    "    d1 = covid_cumulative_cases['cumulative_susceptible']\n",
    "    d2 = covid_cumulative_cases['cumulative_infected1']\n",
    "    d3 = covid_cumulative_cases['cumulative_infected2']\n",
    "    d4 = covid_cumulative_cases['cumulative_dead1']\n",
    "    d5 = covid_cumulative_cases['cumulative_recovered1']\n",
    "    d6 = covid_cumulative_cases['cumulative_dead2']\n",
    "    d7 = covid_cumulative_cases['cumulative_recovered2']\n",
    "    d8 = covid_cumulative_cases['alpha1']\n",
    "\n",
    "    d9 = covid_cumulative_cases['t']\n",
    "\n",
    "    for item in range(len(d9)):\n",
    "        if item % 7 == 0:\n",
    "            cumulative_susceptible.append(d1[item])\n",
    "            cumulative_infected1.append(d2[item])\n",
    "            cumulative_infected2.append(d3[item])\n",
    "            cumulative_dead1.append(d4[item])\n",
    "            cumulative_recovered1.append(d5[item])\n",
    "            cumulative_dead2.append(d6[item])\n",
    "            cumulative_recovered2.append(d7[item])\n",
    "            alpha1.append(d8[item])\n",
    "\n",
    "            timesteps.append(d9[item])\n",
    "\n",
    "    # train\n",
    "\n",
    "    dinn = DINN(timesteps, cumulative_susceptible, cumulative_infected1, cumulative_infected2, cumulative_dead1, cumulative_recovered1, cumulative_dead2, cumulative_recovered2, alpha1) #in the form of [t,S,I,D,R]\n",
    "\n",
    "    learning_rate = 1e-5\n",
    "    optimizer = optim.Adam(dinn.params, lr = learning_rate)\n",
    "    dinn.optimizer = optimizer\n",
    "\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(dinn.optimizer, base_lr=1e-5, max_lr=1e-3, step_size_up=1000, mode=\"exp_range\", gamma=0.85, cycle_momentum=False)\n",
    "\n",
    "    dinn.scheduler = scheduler\n",
    "\n",
    "    try:\n",
    "        S_pred_list, I1_pred_list, I2_pred_list, D1_pred_list, R1_pred_list, D2_pred_list, R2_pred_list = dinn.train(100000) #train\n",
    "    except EOFError:\n",
    "        if dinn.save == 2:\n",
    "            dinn.save = 3\n",
    "            S_pred_list, I1_pred_list, I2_pred_list, D1_pred_list, R1_pred_list, D2_pred_list, R2_pred_list = dinn.train(100000) #train\n",
    "        elif dinn.save == 3:\n",
    "            dinn.save = 2\n",
    "            S_pred_list, I1_pred_list, I2_pred_list, D1_pred_list, R1_pred_list, D2_pred_list, R2_pred_list = dinn.train(100000) #train\n",
    "\n",
    "\n",
    "    # save to csv\n",
    "    df = pd.DataFrame({'S_pred':S_pred_list[0].detach().numpy(),\n",
    "                    'I1_pred':I1_pred_list[0].detach().numpy(),\n",
    "                    'I2_pred':I2_pred_list[0].detach().numpy(),\n",
    "                    'R1_pred':R1_pred_list[0].detach().numpy(),\n",
    "                    'D1_pred':D1_pred_list[0].detach().numpy(),\n",
    "                    'R2_pred':R2_pred_list[0].detach().numpy(),\n",
    "                    'D2_pred':D2_pred_list[0].detach().numpy()},\n",
    "                    index=[covid_cumulative_cases['Date'][int(j)] for j in timesteps])\n",
    "    df.to_csv(f'./csv/DINN_{eng[keys[i]]}.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "interpreter": {
   "hash": "f0396a0f98e081442f6005f4438dae70905c4dba32e635697d7a979ca5a56ea2"
  },
  "kernelspec": {
   "display_name": "KDA_DA",
   "language": "python",
   "name": "kda_da"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
