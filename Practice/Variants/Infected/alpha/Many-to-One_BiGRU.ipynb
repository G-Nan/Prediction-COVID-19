{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef1b38b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T16:02:20.972064Z",
     "start_time": "2023-05-24T16:02:18.605353Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('D:\\\\Gnan\\\\DA\\\\KMU\\\\Prediction-COVID-19')\n",
    "from utils import *\n",
    "from model import *\n",
    "from trainer import *\n",
    "\n",
    "from torch.optim.adam import Adam\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d1bf81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T16:02:21.033901Z",
     "start_time": "2023-05-24T16:02:20.974061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stdDay</th>\n",
       "      <th>incDec</th>\n",
       "      <th>alpha_variants</th>\n",
       "      <th>delta_variants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.16</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-13</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-14</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-15</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2021-08-11</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>62.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>37.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>54.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2021-08-14</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>77.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2021-08-15</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>54.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         stdDay  incDec  alpha_variants  delta_variants\n",
       "0    2021-04-12    16.0            4.16            0.32\n",
       "1    2021-04-13     9.0            2.34            0.18\n",
       "2    2021-04-14    11.0            2.86            0.22\n",
       "3    2021-04-15    18.0            4.68            0.36\n",
       "4    2021-04-16     4.0            1.04            0.08\n",
       "..          ...     ...             ...             ...\n",
       "121  2021-08-11    66.0            0.66           62.70\n",
       "122  2021-08-12    39.0            0.39           37.05\n",
       "123  2021-08-13    57.0            0.57           54.15\n",
       "124  2021-08-14    82.0            0.82           77.90\n",
       "125  2021-08-15    57.0            0.57           54.15\n",
       "\n",
       "[126 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path = 'Data/SIR_Origin/*.csv'\n",
    "#dic_files = Load_files.load_files(path, -17, -15)\n",
    "#path = 'Data/Cleaned_deathCnt/*.csv'\n",
    "#dic_files = Load_files.load_files(path, -14, -12)\n",
    "path = 'Data/Variants/alpha_delta/*.csv'\n",
    "dic_files = Load_files.load_files(path, -18, -16)\n",
    "data = dic_files['대구']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdbfaa5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T16:02:22.660549Z",
     "start_time": "2023-05-24T16:02:21.035895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57, 60, 3])\n",
      "torch.Size([57, 1])\n",
      "torch.Size([7, 60, 3])\n",
      "torch.Size([7, 1])\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "lr = 1e-4\n",
    "patience = 100\n",
    "hidden_size = 32\n",
    "num_layers = 4\n",
    "batch_size = 8\n",
    "dropout = 0.25\n",
    "\n",
    "input_size = 3\n",
    "sequence_length = 60\n",
    "num_epochs = 10000\n",
    "\n",
    "#df = Prepare_df.processing(data, 'Date', 'alpha')\n",
    "df = Prepare_df.processing(data, 'stdDay', 'alpha_variants')\n",
    "\n",
    "x = df.iloc[:, 0:]\n",
    "y = df.iloc[:,:1]\n",
    "\n",
    "ms = MinMaxScaler()\n",
    "ss = StandardScaler()\n",
    "\n",
    "ss.fit(x)\n",
    "ms.fit(y)\n",
    "\n",
    "x, y, x_ss, y_ms, train_loader, test_loader = Prepare_df.split_data(df, 57, 60, 1, batch_size, 'mto')\n",
    "\n",
    "model_BiGRU = BiGRU(input_size = input_size,\n",
    "                      hidden_size = hidden_size,\n",
    "                      sequence_length = sequence_length,\n",
    "                      num_layers = num_layers, \n",
    "                      dropout = dropout, \n",
    "                      device = device).to(device)\n",
    "\n",
    "model = model_BiGRU\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9cb079a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T16:02:23.351700Z",
     "start_time": "2023-05-24T16:02:22.662543Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 4.00 GiB total capacity; 4.67 MiB already allocated; 550.71 MiB free; 22.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss_list, model,\u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMany_to_One\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Gnan\\DA\\KMU\\Prediction-COVID-19\\trainer.py:33\u001b[0m, in \u001b[0;36mTrainer.Many_to_One\u001b[1;34m(train_loader, test_loader, model, criterion, optimizer, num_epochs, patience, device)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     32\u001b[0m     seq, target \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m---> 33\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out, target)\n\u001b[0;32m     36\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KMU_DA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Gnan\\DA\\KMU\\Prediction-COVID-19\\model.py:177\u001b[0m, in \u001b[0;36mBiGRU.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    176\u001b[0m     h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 177\u001b[0m     out, hn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mreshape(out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    179\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(out)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KMU_DA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KMU_DA\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:950\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 950\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    953\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    954\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 4.00 GiB total capacity; 4.67 MiB already allocated; 550.71 MiB free; 22.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "loss_list, model,= Trainer.Many_to_One(train_loader, \n",
    "                                       test_loader, \n",
    "                                       model, \n",
    "                                       criterion, \n",
    "                                       optimizer, \n",
    "                                       num_epochs, \n",
    "                                       patience,\n",
    "                                       device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63bc95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T16:02:23.353695Z",
     "start_time": "2023-05-24T16:02:23.353695Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6009153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T16:02:23.355691Z",
     "start_time": "2023-05-24T16:02:23.355691Z"
    }
   },
   "outputs": [],
   "source": [
    "label_y, predicted = predict_mto(model, df, x_ss, y_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2867cf68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T16:02:23.357685Z",
     "start_time": "2023-05-24T16:02:23.357685Z"
    }
   },
   "outputs": [],
   "source": [
    "print(mape(label_y+1, predicted))\n",
    "plotting(label_y, predicted, 57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849529f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112566a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KDA_DA",
   "language": "python",
   "name": "kda_da"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
